#!/usr/bin/env python3
"""
Consolidate all historical certificate IDs and rescrape them
This ensures we have complete, up-to-date data for all known certificates
"""

import pandas as pd
import asyncio
from pathlib import Path
import sys

def collect_all_historical_ids():
    """Collect all certificate IDs from historical data sources"""
    all_ids = set()

    print("="*80)
    print("ğŸ“š COLLECTING CERTIFICATE IDs FROM ALL SOURCES")
    print("="*80)

    # Source 1: old/2024/certificate_data/*.csv
    print("\n1ï¸âƒ£  old/2024/certificate_data/*.csv files:")
    for year in ['2020', '2021', '2022', '2023', '2024']:
        filepath = Path(f'old/2024/certificate_data/{year}.csv')
        if filepath.exists():
            try:
                df = pd.read_csv(filepath)
                ids = set(df['Certificate ID'].dropna().astype(int).values)
                all_ids.update(ids)
                print(f"   {year}.csv: {len(ids)} IDs")
            except Exception as e:
                print(f"   {year}.csv: Error - {e}")

    # Source 2: old/2022/last.xlsx
    print("\n2ï¸âƒ£  old/2022/last.xlsx:")
    excel_path = Path('old/2022/last.xlsx')
    if excel_path.exists():
        try:
            df = pd.read_excel(excel_path)
            if 'page' in df.columns:
                ids = set(df['page'].dropna().astype(int).values)
                all_ids.update(ids)
                print(f"   Excel file: {len(ids)} IDs")
        except Exception as e:
            print(f"   Excel file: Error - {e}")

    # Source 3: Current data/certificates.csv (to include any already scraped)
    print("\n3ï¸âƒ£  data/certificates.csv (current):")
    current_path = Path('data/certificates.csv')
    if current_path.exists():
        try:
            df = pd.read_csv(current_path)
            ids = set(df['Certificate ID'].dropna().astype(int).values)
            all_ids.update(ids)
            print(f"   Current data: {len(ids)} IDs")
        except Exception as e:
            print(f"   Current data: Error - {e}")

    # Source 4: Add discovered pattern ranges that might not be in historical data
    print("\n4ï¸âƒ£  Adding known pattern ranges:")

    # New 5-digit range
    new_5digit = set(range(20000, 21000))
    print(f"   5-digit range (20000-20999): {len(new_5digit)} IDs")
    all_ids.update(new_5digit)

    # New 6-digit range
    new_6digit = set(range(202000, 204000))
    print(f"   6-digit range (202000-203999): {len(new_6digit)} IDs")
    all_ids.update(new_6digit)

    # 2025 legacy pattern
    pattern_2025 = set(range(2025001, 2026000))
    print(f"   2025 pattern (2025001-2025999): {len(pattern_2025)} IDs")
    all_ids.update(pattern_2025)

    # 2026 future-proofing
    pattern_2026 = set(range(2026001, 2027000))
    print(f"   2026 pattern (2026001-2026999): {len(pattern_2026)} IDs")
    all_ids.update(pattern_2026)

    # Legacy patterns (ensure complete coverage)
    legacy_patterns = [
        (2011101, 2011994),  # 2020
        (2103599, 2103717),  # 2021
        (2022001, 2022995),  # 2022
        (2023101, 2023999),  # 2023
        (2024101, 2024999),  # 2024
    ]

    print(f"\n   Legacy 7-digit patterns:")
    for start, end in legacy_patterns:
        pattern_ids = set(range(start, end + 1))
        all_ids.update(pattern_ids)
        print(f"     {start:,} - {end:,}: {len(pattern_ids)} IDs")

    # Add the missing range from old Excel file
    missing_range_1 = set(range(2021763, 2021930))
    print(f"\n   Missing Excel range (2021763-2021929): {len(missing_range_1)} IDs")
    all_ids.update(missing_range_1)

    return all_ids

def save_consolidated_ids(all_ids):
    """Save consolidated list of IDs to file"""
    output_file = Path('data/consolidated_ids.txt')

    sorted_ids = sorted(all_ids)

    with open(output_file, 'w') as f:
        f.write("# Consolidated Certificate IDs from all sources\n")
        f.write(f"# Total unique IDs: {len(sorted_ids)}\n")
        f.write("# Generated by consolidate_and_rescrape.py\n")
        f.write("#\n")
        for id in sorted_ids:
            f.write(f"{id}\n")

    print(f"\nâœ… Saved {len(sorted_ids)} unique IDs to: {output_file}")
    return output_file

def create_scraping_batches(all_ids, batch_size=10000):
    """Create batches of IDs for efficient scraping"""
    sorted_ids = sorted(all_ids)

    print(f"\nğŸ“¦ Creating scraping batches (max {batch_size} IDs per batch):")

    batches = []
    for i in range(0, len(sorted_ids), batch_size):
        batch = sorted_ids[i:i+batch_size]
        start = batch[0]
        end = batch[-1]
        batches.append((start, end, len(batch)))
        print(f"   Batch {len(batches)}: {start:,} - {end:,} ({len(batch)} IDs)")

    return batches

def main():
    """Main consolidation function"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          CONSOLIDATE & RESCRAPE ALL HISTORICAL CERTIFICATES               â•‘
â•‘                                                                           â•‘
â•‘  This script will:                                                        â•‘
â•‘  1. Collect all certificate IDs from historical sources                  â•‘
â•‘  2. Deduplicate them                                                      â•‘
â•‘  3. Create an optimized rescraping plan                                   â•‘
â•‘  4. Execute comprehensive rescrape                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # Step 1: Collect all IDs
    all_ids = collect_all_historical_ids()

    print(f"\n{'='*80}")
    print("ğŸ“Š CONSOLIDATION SUMMARY")
    print(f"{'='*80}")
    print(f"Total unique certificate IDs: {len(all_ids):,}")
    print(f"ID range: {min(all_ids):,} - {max(all_ids):,}")

    # Step 2: Save consolidated list
    consolidated_file = save_consolidated_ids(all_ids)

    # Step 3: Analyze ID distribution
    print(f"\n{'='*80}")
    print("ğŸ“ˆ ID DISTRIBUTION ANALYSIS")
    print(f"{'='*80}")

    id_ranges = {
        "5-digit (20XXX)": len([x for x in all_ids if 20000 <= x < 21000]),
        "6-digit (202XXX-203XXX)": len([x for x in all_ids if 202000 <= x < 204000]),
        "7-digit 2011XXX": len([x for x in all_ids if 2011000 <= x < 2012000]),
        "7-digit 2021XXX": len([x for x in all_ids if 2021000 <= x < 2022000]),
        "7-digit 2022XXX": len([x for x in all_ids if 2022000 <= x < 2023000]),
        "7-digit 2023XXX": len([x for x in all_ids if 2023000 <= x < 2024000]),
        "7-digit 2024XXX": len([x for x in all_ids if 2024000 <= x < 2025000]),
        "7-digit 2025XXX": len([x for x in all_ids if 2025000 <= x < 2026000]),
        "7-digit 2026XXX": len([x for x in all_ids if 2026000 <= x < 2027000]),
        "7-digit 2103XXX": len([x for x in all_ids if 2103000 <= x < 2104000]),
    }

    for range_name, count in id_ranges.items():
        if count > 0:
            print(f"  {range_name}: {count:,} IDs")

    # Step 4: Create batches
    batches = create_scraping_batches(all_ids, batch_size=5000)

    # Step 5: Generate scraping commands
    print(f"\n{'='*80}")
    print("ğŸš€ READY TO RESCRAPE")
    print(f"{'='*80}")

    print(f"\nTotal IDs to scrape: {len(all_ids):,}")
    print(f"Estimated time: ~{len(all_ids) / 120 / 60:.1f} minutes @ 120 req/s")
    print(f"\nOption 1 - Scrape by continuous ranges (RECOMMENDED):")
    print(f"  This will use the checkpoint system to skip already-processed IDs\n")

    # Find continuous ranges
    sorted_ids = sorted(all_ids)
    ranges = []
    start = sorted_ids[0]
    prev = sorted_ids[0]

    for id in sorted_ids[1:]:
        if id > prev + 100:  # Gap of more than 100 IDs
            ranges.append((start, prev))
            start = id
        prev = id
    ranges.append((start, prev))

    print("  Commands to run:")
    for i, (start, end) in enumerate(ranges, 1):
        count = end - start + 1
        if count >= 100:  # Only show significant ranges
            print(f"  python scripts/scraper.py --start {start} --end {end} --concurrent 50")

    print(f"\nOption 2 - Scrape ALL ranges in one comprehensive sweep:")
    print(f"  python scripts/scraper.py --start {min(all_ids)} --end {max(all_ids)} --concurrent 50")
    print(f"  âš ï¸  Note: This will check many IDs that don't exist (404s)")

    print(f"\nOption 3 - Use the optimized consolidated scraper (coming next):")
    print(f"  python scripts/scrape_consolidated.py")

    return all_ids, batches

if __name__ == "__main__":
    all_ids, batches = main()

    print(f"\n{'='*80}")
    print("âœ… CONSOLIDATION COMPLETE")
    print(f"{'='*80}")
    print(f"\nConsolidated list saved to: data/consolidated_ids.txt")
    print(f"Total unique IDs: {len(all_ids):,}")
    print(f"\nNext step: Run the scraper commands shown above")
    print(f"           or use: python scripts/scrape_consolidated.py")
